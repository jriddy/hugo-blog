---
title: "Data Driven Perils"
date: 2023-09-29T07:25:10-04:00
draft: true

user-outline:
  - Introduction:
    - Seeingly ubiquitous at tech companies
    - Is it a case of "Google does it"?
  - Hard to measure right data:
    - Unit problem
    - What is easy to measure is not necessarily what is useful to know
  - People and systems react to being measured
  - Post-hoc narratives dominate anyways
  - Conclusion - What works?:
    -

---

Tech companies aspire to be _data-driven_.  It's almost a foregone conclusion at this point.  This
is the default Is it a clich√© yet that all tech companies seek to be data-driven?  If not, it
should be.  Your average tech exec certainly wouldn't want to be caught _not_ data-driven,
following some squishy humanistic _narrative_ over _cold, hard logical facts_.  At least, not
unless they had some guiding philosophy that suggested otherwise.

This desire to be (or at least _appear to be_) data-driven does little to change what
decisions are made at these organizations, and usually only exists to defend management
from criticism.

Numbers give the appearance of science.  The old saw is that 70% of statistics are made up on the
spot, with that statistic itself inevitably being made up on the spot.  But the sense behind that
aphorism works because we all intuitively understand how just adding a plausible number to data can
bypass our critical thinking and get us to accept it as facts.   In public discourse, you can get
caught using bad data, since there do exist fact-checkers.  And if you're truly making up data,
it's pretty easy to call someone on it in casual conversation.   But in the context of
organizational meetings backed by dashboards derived from data that has ostensibly been collected
from _somewhere_, it's a lot harder to challenge the validity of the data on the spot.

This is made worse because you never know which data point someone will rest their narrative on,
and how valid that particular data point is.

## It's hard to measure the right data

People sometimes assume that any data is good, or that it's better than no data.  This discounts
the fact that data can be _bad_ or _wrong_.  By _bad data_, I mean low quality or low resolution
data that suffers from collection or sampling issues that make it fundamentally incorrect in some
way.  Bad data simply doesn't properly represent the ground reality it purports to.  _Wrong_ data
is data that is either not pertinent or not specific, but lends itself to drawing conclusions that
are unwarranted.  Both cases are insidious in that the presence of data tends to turn off critical
reasoning about that data, and we tend to just assume that it is valid as-is.

But getting _good_ data displayed in the _right way_ remains a subtle and difficult problem.  The
trap here is how easy it is to get started. Modern tools make it absurdly easy to put together a
dashboard, which can make for a snappy presentation or cool operations workstation look.  But
nothing about our tools helps us judge the quality or relevance of our data to the goals we want to
apply it towards.
